{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz \n",
    "from fuzzywuzzy import process \n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.read_csv('outfit_combinations.csv',encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " d=d.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating brand tokens\n",
    "\n",
    "d['brand'] = d['brand'].apply(unidecode) ## converts accented characters to normal characters. \n",
    "                                         ## Otherwise this would create problem in similarity calc\n",
    "regex = re.compile('[^a-zA-Z\\s]')\n",
    "d['brand'] = d['brand'].str.replace(regex,'')\n",
    "brands=[]\n",
    "for value in d.brand:\n",
    "    for value in re.findall(r\"[\\w']+\", value):\n",
    "        brands.append(value.lower())\n",
    "brands=set(brands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating outfit type tokens\n",
    "\n",
    "d['outfit_item_type']=d['outfit_item_type'].str.replace(r'\\d+','')\n",
    "outfit_types=[]\n",
    "for value in d.outfit_item_type:\n",
    "    for value in re.findall(r\"[\\w']+\", value):\n",
    "        outfit_types.append(value.lower())\n",
    "outfit_types=set(outfit_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating product_full_name tokens\n",
    "d['product_full_name'] = d['product_full_name'].apply(unidecode) ## converts accented characters to normal characters. \n",
    "                                         ## Otherwise this would create problem in similarity calc\n",
    "d['product_full_name'] = d['product_full_name'].str.replace(regex,'')\n",
    "product_full_names=[]\n",
    "for value in d.product_full_name:\n",
    "    for value in re.findall(r\"[\\w']+\", value):\n",
    "        product_full_names.append(value.lower())\n",
    "product_full_names=set(product_full_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26    5291\n",
       "Name: oi_len, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['oi_len']=d['outfit_id'].apply(len)\n",
    "d.oi_len.value_counts()\n",
    "#From this we can figure out that if the query is of 26 characters or more and is a alphanumeric string\n",
    "#then we can assume that its a product ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using basic FuzzyWuzzy matching for product ID\n",
    "def returnFuzzyMatches(query,choices):\n",
    "        result=process.extractOne(query, choices,scorer=fuzz.token_sort_ratio)\n",
    "        score=result[1]\n",
    "        matches=[]\n",
    "        if score==100: ## perfect match, then\n",
    "            matches.append(result[0])  \n",
    "        else:\n",
    "            list_of_matches=process.extract(query, choices, limit=5)\n",
    "            for match in list_of_matches:\n",
    "                matches.append(match[0])\n",
    "        return matches\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters,punctuations,spaces. Concat all the words in the query string\n",
    "#This takes care of typos in the first place. Also we only need either numbers or characters \n",
    "#for any searching purpose\n",
    "def refinedQueryString(query):\n",
    "    return ''.join(e for e in query if e.isalnum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinationsByOutfitIds(ids,d):\n",
    "    combinations={}\n",
    "    for id in ids:\n",
    "        combinations[id]=d.loc[d['outfit_id'] == id]\n",
    "    return combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCombinations(combinations):\n",
    "    for outfidId,combination in combinations.items():\n",
    "        print('For Outfit ID: ',outfidId)\n",
    "        for index,row in combination.iterrows():\n",
    "            print(row.outfit_item_type,':',row.product_full_name,'(',row.product_id,')')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchedOutfitsByProductIds(ids,d):\n",
    "    matchedOutfits=[]\n",
    "    for id in ids:\n",
    "        matchedOutfits.append(str(d.loc[d.product_id==id]['outfit_id'].values[0]))\n",
    "    return matchedOutfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDfByBrand(query_tokens,filtered_df):\n",
    "    brand_match=brands.intersection(set(query_tokens))\n",
    "    brand_match_df=filtered_df.loc[:, 'brand'].str.contains(r'\\b(?:{})\\b'.format('|'.join(list(brand_match))))\n",
    "    return filtered_df[brand_match_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function tokenizes the query. Checks if any token matches any brand tokens, \n",
    "#if yes, then filter the dataset by brand.\n",
    "#then checks for outfit_type\n",
    "#if yes, then filter the dataset by outfit_type\n",
    "def findSimilarProducts(query,d):\n",
    "    query_tokens=[]\n",
    "    filtered_df=d.copy()\n",
    "    for value in re.findall(r\"[\\w']+\", query):\n",
    "        query_tokens.append(value.lower())\n",
    "    dfByBrand=getDfByBrand(query_tokens,filtered_df)\n",
    "    if(len(dfByBrand)>0):\n",
    "        filtered_df=dfByBrand.copy()\n",
    "        \n",
    "    ##concatenate all the columns\n",
    "    filtered_df=filtered_df.astype(str).reset_index()\n",
    "    concat_filtered_df=filtered_df.iloc[:, 2:-1].apply(' '.join, 1).reset_index().drop(columns=['index'])\n",
    "    concat_filtered_df.columns=['concatenated_columns']\n",
    "    dataset_list=concat_filtered_df.concatenated_columns.tolist()\n",
    "   \n",
    "    ## vectorize the dataset and the get word embeddings\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit(dataset_list)\n",
    "    dataset_vectors = getVectors(dataset_list,X,vectorizer)\n",
    "\n",
    "    ## vectorize the query\n",
    "    query_vectors = getVectors([query],X,vectorizer)\n",
    "    query_vectors = [list(query_vectors[0])]\n",
    "    \n",
    "    ##find the similarity\n",
    "    sim_score = []\n",
    "    for i in range(filtered_df.shape[0]):\n",
    "        dataset_vectors[i] = list(dataset_vectors[i])\n",
    "        score =  float(cosine_similarity([dataset_vectors[i]],query_vectors))\n",
    "        sim_score.append(score)\n",
    "\n",
    "    max_row = sim_score.index(max(sim_score))\n",
    "    return filtered_df.loc[max_row,\"product_id\"]\n",
    "    #TO:DO add outfit_type filter\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(data_list,X,vectorizer):\n",
    "    nlp = en_core_web_md.load()\n",
    "    X = X.transform(data_list)\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "    # sum the tf idf scores for each document\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns # a list of all the columns we have\n",
    "    available_tf_idf_scores = list(map( lambda x: x.lower(), available_tf_idf_scores)) # lowercase everything\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    vectors = []\n",
    "    for idx, doc in enumerate(data_list): # iterate through each review\n",
    "        tokens = nlp(doc) # have spacy tokenize the review text\n",
    "\n",
    "        # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "\n",
    "        # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "\n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "\n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "                #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "\n",
    "        # divide the total embedding by the total tf-idf score for each document\n",
    "        document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        vectors.append(document_embedding)\n",
    "    return vectors\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendOutfit(query,d):\n",
    "    refineQueryString=''\n",
    "    refineQueryString=refinedQueryString(query)\n",
    "    if refineQueryString.isalnum()==True and len(refineQueryString) >= 26:\n",
    "        #Look for product Id\n",
    "        df=d.iloc[:, 1].apply(''.join, 1).reset_index()\n",
    "        df.drop(columns=['index'],inplace=True)\n",
    "        df.columns=['X']\n",
    "        choices = df.X.tolist()\n",
    "        productIDs=returnFuzzyMatches(query,choices)\n",
    "        matchedOutfitIds=set(getMatchedOutfitsByProductIds(productIDs,d))\n",
    "        combinations=getCombinationsByOutfitIds(matchedOutfitIds,d)\n",
    "        printCombinations(combinations)\n",
    "    else:\n",
    "        matchedOutfitIds=set(getMatchedOutfitsByProductIds([findSimilarProducts(query,d)],d))\n",
    "        combinations=getCombinationsByOutfitIds(matchedOutfitIds,d)\n",
    "        printCombinations(combinations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Product ID or any textfenimore double buckle shoes\n"
     ]
    }
   ],
   "source": [
    "query=input('input Product ID or any text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fenimore double buckle shoes'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Outfit ID:  01e6mc52dnja6twpz2v4pb4dps\n",
      "shoe : fenimore triple buckle boot ( 01e2p0sjskfknqj5svq8md1jzt )\n",
      "onepiece : darcy dress ( 01e4ehhmc6yp74e9j8qv3fr4cw )\n",
      "accessory : leather circle crossbody bag ( 01e5zs3r9jd696ywgk9nsg56e1 )\n",
      "accessory : mm gradient oversize square sunglasses ( 01e5zyhza7186dvwej99q4d2pm )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendOutfit(query,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
